{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bit31a300f32e4a4979894c2ccf59316df7",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Training a character LSTM Neural Network\n",
    "\n",
    "In this python notebook, I will be preprocessing and training a character based neural network. Although spoiler alert: it does not work well for this task.\n",
    "\n",
    "The goal of this model is to give an input name, and generate a predicted trump nickname.\n",
    "\n",
    "# Lets start with preprocessing!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       fake name           real name  len fake  len real  \\\n",
       "0          dumbo  randolph tex alles         1         3   \n",
       "1  wheres hunter        hunter biden         2         2   \n",
       "2         1% joe           joe biden         2         2   \n",
       "3   basement joe           joe biden         3         2   \n",
       "4    beijing joe           joe biden         3         2   \n",
       "\n",
       "                     category  \\\n",
       "0  domestic political figures   \n",
       "1  domestic political figures   \n",
       "2  domestic political figures   \n",
       "3  domestic political figures   \n",
       "4  domestic political figures   \n",
       "\n",
       "                                               notes  count  \n",
       "0       director of the united states secret service      1  \n",
       "1  american lawyer and lobbyist who is the second...      1  \n",
       "2  47th vice president of the united states; form...      1  \n",
       "3  47th vice president of the united states; form...      1  \n",
       "4  47th vice president of the united states; form...      1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fake name</th>\n      <th>real name</th>\n      <th>len fake</th>\n      <th>len real</th>\n      <th>category</th>\n      <th>notes</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>dumbo</td>\n      <td>randolph tex alles</td>\n      <td>1</td>\n      <td>3</td>\n      <td>domestic political figures</td>\n      <td>director of the united states secret service</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>wheres hunter</td>\n      <td>hunter biden</td>\n      <td>2</td>\n      <td>2</td>\n      <td>domestic political figures</td>\n      <td>american lawyer and lobbyist who is the second...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1% joe</td>\n      <td>joe biden</td>\n      <td>2</td>\n      <td>2</td>\n      <td>domestic political figures</td>\n      <td>47th vice president of the united states; form...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>basement joe</td>\n      <td>joe biden</td>\n      <td>3</td>\n      <td>2</td>\n      <td>domestic political figures</td>\n      <td>47th vice president of the united states; form...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>beijing joe</td>\n      <td>joe biden</td>\n      <td>3</td>\n      <td>2</td>\n      <td>domestic political figures</td>\n      <td>47th vice president of the united states; form...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "raw = pd.read_csv('cleaned.nicknames.csv')\n",
    "\n",
    "model = spacy.load('en_core_web_sm')\n",
    "\n",
    "# add period so training knows when its done\n",
    "nicknames = [f'{name}.' for name in raw['fake name']]\n",
    "realnames = [f'{name}' for name in raw['real name']]\n",
    "\n",
    "raw.head()"
   ]
  },
  {
   "source": [
    "Here we simply loaded the data and created two lists 'nicknames' and 'realnames' which will be used for vectorizing.\n",
    "\n",
    "However, first lets grab the part of speech for the nicknames.\n",
    "\n",
    "# Grabbing POS from nicknames"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{dumbo: 'PROPN'},\n",
       " {where: 'ADV', s: 'PROPN', hunter: 'VERB'},\n",
       " {1: 'NUM', %: 'NOUN', joe: 'PROPN'}]"
      ]
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "source": [
    "# pos tagging\n",
    "def nlp(name):\n",
    "    name = model(name)\n",
    "    return {word: word.pos_ for word in name}\n",
    "\n",
    "test = [nlp(name) for name in raw['fake name']]\n",
    "\n",
    "test[0:3]\n",
    "\n",
    "# for name in test:\n",
    "#     print(name)\n",
    "#     for token in name:\n",
    "#         print(token, token.pos_)"
   ]
  },
  {
   "source": [
    "These are the first three nicknames with their part of speech. One thing I notice is that all prefixes are not adjectives.\n",
    "\n",
    "We will not actually use these in the training, I was simply curious about the tags.\n",
    "\n",
    "# Lets begin vectorizing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['dumbo.', 'wheres hunter.', '1% joe.'] ['randolph tex alles', 'hunter biden', 'joe biden']\n"
     ]
    }
   ],
   "source": [
    "print(nicknames[0:3], realnames[0:3])"
   ]
  },
  {
   "source": [
    "The first list is the nicknames, as you can see we added a period at the end of each name. This gives us a marked character for the end of the name, which will be useful for prediction and generation.\n",
    "\n",
    "Now lets vectorize!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# vectorize names\n",
    "##########################\n",
    "allnames = nicknames + realnames\n",
    "\n",
    "# max char length in names\n",
    "max_chars = max([len(name) for name in allnames])\n",
    "# max_real_chars = max([len(name) for name in realnames])\n",
    "# total number of names\n",
    "n = len(nicknames)\n",
    "# nicknames to realnames\n",
    "nick2real = {nicknames[i]:realnames[i] for i in range(n)}\n",
    "# character to index\n",
    "char2i = {char:0 for name in allnames for char in name}\n",
    "char2i = {char:n for n, char in enumerate(char2i)}\n",
    "# index to character\n",
    "i2char = {char2i[char]: char for char in char2i}\n",
    "char_dimensions = len(i2char)\n",
    "\n",
    "# set up vectors for output = nicknames\n",
    "output = np.zeros((n, max_chars, char_dimensions))\n",
    "# set up vectors for label = real names\n",
    "label = np.zeros((n, max_chars, char_dimensions))\n",
    "\n",
    "# vectorize output and labels\n",
    "for i, name in enumerate(nicknames):\n",
    "    name = list(name)\n",
    "    for row, ch in enumerate(name):\n",
    "        # assign 1 to nickname, character number, vocab index\n",
    "        label[i, row, char2i[ch]] = 1\n",
    "        \n",
    "    for row, ch in enumerate(nick2real[''.join(name)]):\n",
    "        # assign 1 to real name, character number, vocab index\n",
    "        output[i, row, char2i[ch]] = 1\n",
    "\n",
    "print(output[0])"
   ]
  },
  {
   "source": [
    "As we can see, this is the first nickname that was vectorized. This follows the pattern of having a direct diagnal line of 1's. This is due to our vocabulary setup, as the vocab was pulled directly from our characters, instead of creating a vocabulary on our own.\n",
    "\n",
    "Either way, we know this worked, so lets move on to training!\n",
    "\n",
    "# Training the neural network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<tensorflow.python.keras.engine.sequential.Sequential object at 0x000001F01C971B50>\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(max_chars, char_dimensions), return_sequences=True))\n",
    "model.add(Dense(char_dimensions, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print(model)"
   ]
  },
  {
   "source": [
    "As we can see, we generated the neural model.\n",
    "\n",
    "As we do not care about accuracy in the traditional sense for this task, I will build a simple function to generate a name based on the model probabilities. This will be used to generate names at every designated epoch."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name(model, limit, input):\n",
    "    # vectorize the input\n",
    "    word_vec = np.zeros((1, max_chars, char_dimensions))\n",
    "\n",
    "    def predict(index):\n",
    "        # pull probabilities for character index\n",
    "        probabilities = list(model.predict(word_vec)[0,index])\n",
    "        # normalize probabilities\n",
    "        probabilities = probabilities / np.sum(probabilities)\n",
    "        if index == limit-1:\n",
    "            return '.'\n",
    "        # guess a letter\n",
    "        guess = np.random.choice(range(char_dimensions), p=probabilities) # choose a letter\n",
    "        word_vec[0, index+1, guess] = 1\n",
    "        return i2char[guess]\n",
    "\n",
    "\n",
    "    gen_name = ''.join([predict(i) for i in range(limit)])\n",
    "    print(f'{input}: {gen_name}')"
   ]
  },
  {
   "source": [
    "# Training\n",
    "\n",
    "Now lets run our model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Names generated after epoch 0:\n",
      "donald trump: car dllllln .\n",
      "joe biden: sal aaasssee.\n",
      "hillary clinton: eil c denee..\n",
      "\n",
      "Names generated after epoch 999:\n",
      "donald trump: cae ci olyyy.\n",
      "joe biden: mlai   litff.\n",
      "hillary clinton: baenmicollce.\n",
      "\n",
      "Names generated after epoch 1998:\n",
      "donald trump: jlatty enn e.\n",
      "joe biden: cae cc lll y.\n",
      "hillary clinton: shunpeeclirg.\n",
      "\n",
      "Names generated after epoch 2997:\n",
      "donald trump: hhrerhhyetle.\n",
      "joe biden: mlooy dann.l.\n",
      "hillary clinton: bla ii ooad .\n",
      "\n",
      "Names generated after epoch 3996:\n",
      "donald trump: shunpppr  oo.\n",
      "joe biden: shutpppl ce .\n",
      "hillary clinton: cay  kalana..\n",
      "\n",
      "Names generated after epoch 4995:\n",
      "donald trump: mrai yylekd..\n",
      "joe biden: shunpppri .o.\n",
      "hillary clinton: mdai   ah te.\n",
      "\n",
      "Names generated after epoch 5994:\n",
      "donald trump: tii   r  aee.\n",
      "joe biden: bah  illlnke.\n",
      "hillary clinton: sso b   lans.\n",
      "\n",
      "Names generated after epoch 6993:\n",
      "donald trump: mddi   rin h.\n",
      "joe biden: tii thpppeee.\n",
      "hillary clinton: pao teennnaa.\n",
      "\n",
      "Names generated after epoch 7992:\n",
      "donald trump: mlooy denn.y.\n",
      "joe biden: mlooy heneel.\n",
      "hillary clinton: shunpp r   f.\n",
      "\n",
      "Names generated after epoch 8991:\n",
      "donald trump: dlakpp  mnce.\n",
      "joe biden: tii  alimaan.\n",
      "hillary clinton: mlooy deneey.\n",
      "\n",
      "Names generated after epoch 9990:\n",
      "donald trump: car dlalaand.\n",
      "joe biden: tii ttnrpeee.\n",
      "hillary clinton: laegyy iieea.\n",
      "\n",
      "INFO:tensorflow:Assets written to: character.model.output\\assets\n"
     ]
    }
   ],
   "source": [
    "def generate_name_loop(epoch, _):\n",
    "    if epoch % 999 == 0:\n",
    "        \n",
    "        print('Names generated after epoch %d:' % epoch)\n",
    "\n",
    "        for i, name in enumerate(['donald trump', 'joe biden', 'hillary clinton']):\n",
    "            generate_name(model, limit = 13, input = name)\n",
    "        \n",
    "        print()\n",
    "      \n",
    "name_generator = LambdaCallback(on_epoch_end = generate_name_loop)\n",
    "\n",
    "model.fit(output, label, batch_size=64, epochs=10000, callbacks=[name_generator], verbose=0)\n",
    "\n",
    "model.save(\"character.model.output\")"
   ]
  },
  {
   "source": [
    "Things to note about this training:\n",
    "\n",
    "+ no name was less than the max characters\n",
    "    - this means that every generation never predicted the end of the name itself, this might be able to be rectified using the probabilities based on our data set. However, this is a major limiter into the useability and generation of the names.\n",
    "\n",
    "+ we see names with more than one consecutive space\n",
    "    - this could easily be corrected by hardcoding a limit to one consecutive space in the name generation, however, it is not worth the effort for this model.\n",
    "\n",
    "+ the generated names tend to become more word-like, showing a good understanding of how words are put together.\n",
    "    - However, the biggest problem we see is repeated characters. This could also be hardcoded, but would limit a name that has consecutive letters, such as 'hillary'.\n",
    "    \n",
    "As we can see from the epoch generations, the model does not work well at all. However, I am going to save the model to generate some names and evaluate how well it did.\n",
    "\n",
    "# Model Evaluation\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "limit: 5 --------\n",
      "alex kahanek: can .\n",
      "donald trump: baee.\n",
      "joe biden: cay .\n",
      "barack obama: banm.\n",
      "george bush: mloo.\n",
      "hillary clinton: laaa.\n",
      "karen: cle .\n",
      "bob: tii .\n",
      "the one guy: mdai.\n",
      "dumbo: paop.\n",
      "1% joe: laog.\n",
      "this name is really long: laaa.\n",
      "-----------------------\n",
      "\n",
      "\n",
      "limit: 10 --------\n",
      "alex kahanek: feeckett .\n",
      "donald trump: shuupppo .\n",
      "joe biden: mlai   il.\n",
      "barack obama: rduuu rr .\n",
      "george bush: cae c tel.\n",
      "hillary clinton: mddi   oo.\n",
      "karen: tii tuppp.\n",
      "bob: sno   lll.\n",
      "the one guy: mhoooepde.\n",
      "dumbo: rda py te.\n",
      "1% joe: laaooy li.\n",
      "this name is really long: tio   nr .\n",
      "-----------------------\n",
      "\n",
      "\n",
      "limit: 15 --------\n",
      "alex kahanek: ghrttyl madn...\n",
      "donald trump: laaayy coniiii.\n",
      "joe biden: dlaaay dannteg.\n",
      "barack obama: tii  lo  eanye.\n",
      "george bush: banecc moee h..\n",
      "hillary clinton: cam c  llt haa.\n",
      "karen: cae c  lln  yn.\n",
      "bob: cae c   lethhr.\n",
      "the one guy: mloo  han eece.\n",
      "dumbo: blhkpploeeepan.\n",
      "1% joe: mllo  ae dedlm.\n",
      "this name is really long: laegy  iinehh..\n",
      "-----------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model(\"character.model.output\")\n",
    "\n",
    "names = 'alex kahanek,donald trump,joe biden,barack obama,george bush,hillary clinton,karen,bob,the one guy,dumbo,1% joe,this name is really long'.split(',')\n",
    "name_limit = [5, 10, 15]\n",
    "\n",
    "for limit in name_limit:\n",
    "    print(f'limit: {limit} --------')\n",
    "    for name in names:\n",
    "        generate_name(model, limit = limit, input = name)\n",
    "    print('-----------------------')\n",
    "    print('\\n')"
   ]
  },
  {
   "source": [
    "From this we can see that the character based name generation did not work well. It seems as though there is not enough training data to generate coherent names, as such I think a word based model might perform better for this task.\n",
    "\n",
    "Notes about why this is bad:\n",
    "\n",
    "+ as the limit increases, we see more repeated characters\n",
    "+ the names never stop themselves\n",
    "+ the algorithm really likes the letter j\n",
    "+ none of the names make any sense\n",
    "\n",
    "Notes about what is good:\n",
    "\n",
    "+ notice that if you took out the repeat letters, the names do resemble words more.\n",
    "+ it does seem to understand the difference and usage of vowels and constants,\n",
    "    - if we ignore the repeat letters.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}