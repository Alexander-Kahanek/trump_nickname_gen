{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bit31a300f32e4a4979894c2ccf59316df7",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Training a character LSTM Neural Network\n",
    "\n",
    "In this python notebook, I will be preprocessing and training a character based neural network. Although spoiler alert: it does not work well for this task.\n",
    "\n",
    "The goal of this model is to give an input name, and generate a predicted trump nickname.\n",
    "\n",
    "# Lets start with preprocessing!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       fake name           real name  len fake  len real  \\\n",
       "0          dumbo  randolph tex alles         1         3   \n",
       "1  wheres hunter        hunter biden         2         2   \n",
       "2         1% joe           joe biden         2         2   \n",
       "3   basement joe           joe biden         3         2   \n",
       "4    beijing joe           joe biden         3         2   \n",
       "\n",
       "                     category  \\\n",
       "0  domestic political figures   \n",
       "1  domestic political figures   \n",
       "2  domestic political figures   \n",
       "3  domestic political figures   \n",
       "4  domestic political figures   \n",
       "\n",
       "                                               notes  count  \n",
       "0       director of the united states secret service      1  \n",
       "1  american lawyer and lobbyist who is the second...      1  \n",
       "2  47th vice president of the united states; form...      1  \n",
       "3  47th vice president of the united states; form...      1  \n",
       "4  47th vice president of the united states; form...      1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fake name</th>\n      <th>real name</th>\n      <th>len fake</th>\n      <th>len real</th>\n      <th>category</th>\n      <th>notes</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>dumbo</td>\n      <td>randolph tex alles</td>\n      <td>1</td>\n      <td>3</td>\n      <td>domestic political figures</td>\n      <td>director of the united states secret service</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>wheres hunter</td>\n      <td>hunter biden</td>\n      <td>2</td>\n      <td>2</td>\n      <td>domestic political figures</td>\n      <td>american lawyer and lobbyist who is the second...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1% joe</td>\n      <td>joe biden</td>\n      <td>2</td>\n      <td>2</td>\n      <td>domestic political figures</td>\n      <td>47th vice president of the united states; form...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>basement joe</td>\n      <td>joe biden</td>\n      <td>3</td>\n      <td>2</td>\n      <td>domestic political figures</td>\n      <td>47th vice president of the united states; form...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>beijing joe</td>\n      <td>joe biden</td>\n      <td>3</td>\n      <td>2</td>\n      <td>domestic political figures</td>\n      <td>47th vice president of the united states; form...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "raw = pd.read_csv('cleaned.nicknames.csv')\n",
    "\n",
    "model = spacy.load('en_core_web_sm')\n",
    "\n",
    "# add period so training knows when its done\n",
    "nicknames = [f'{name}.' for name in raw['fake name']]\n",
    "realnames = [f'{name}' for name in raw['real name']]\n",
    "\n",
    "raw.head()"
   ]
  },
  {
   "source": [
    "Here we simply loaded the data and created two lists 'nicknames' and 'realnames' which will be used for vectorizing.\n",
    "\n",
    "However, first lets grab the part of speech for the nicknames.\n",
    "\n",
    "# Grabbing POS from nicknames"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{dumbo: 'PROPN'},\n",
       " {where: 'ADV', s: 'PROPN', hunter: 'VERB'},\n",
       " {1: 'NUM', %: 'NOUN', joe: 'PROPN'}]"
      ]
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "source": [
    "# pos tagging\n",
    "def nlp(name):\n",
    "    name = model(name)\n",
    "    return {word: word.pos_ for word in name}\n",
    "\n",
    "test = [nlp(name) for name in raw['fake name']]\n",
    "\n",
    "test[0:3]\n",
    "\n",
    "# for name in test:\n",
    "#     print(name)\n",
    "#     for token in name:\n",
    "#         print(token, token.pos_)"
   ]
  },
  {
   "source": [
    "These are the first three nicknames with their part of speech. One thing I notice is that all prefixes are not adjectives.\n",
    "\n",
    "We will not actually use these in the training, I was simply curious about the tags.\n",
    "\n",
    "# Lets begin vectorizing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['dumbo.', 'wheres hunter.', '1% joe.'] ['randolph tex alles', 'hunter biden', 'joe biden']\n"
     ]
    }
   ],
   "source": [
    "print(nicknames[0:3], realnames[0:3])"
   ]
  },
  {
   "source": [
    "The first list is the nicknames, as you can see we added a period at the end of each name. This gives us a marked character for the end of the name, which will be useful for prediction and generation.\n",
    "\n",
    "Now lets vectorize!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n [0. 1. 0. ... 0. 0. 0.]\n [0. 0. 1. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# vectorize names\n",
    "##########################\n",
    "allnames = nicknames + realnames\n",
    "\n",
    "# max char length in names\n",
    "max_chars = max([len(name) for name in allnames])\n",
    "# max_real_chars = max([len(name) for name in realnames])\n",
    "# total number of names\n",
    "n = len(nicknames)\n",
    "# nicknames to realnames\n",
    "nick2real = {nicknames[i]:realnames[i] for i in range(n)}\n",
    "# character to index\n",
    "char2i = {char:0 for name in allnames for char in name}\n",
    "char2i = {char:n for n, char in enumerate(char2i)}\n",
    "# index to character\n",
    "i2char = {char2i[char]: char for char in char2i}\n",
    "char_dimensions = len(i2char)\n",
    "\n",
    "# set up vectors for output = nicknames\n",
    "output = np.zeros((n, max_chars, char_dimensions))\n",
    "# set up vectors for label = real names\n",
    "label = np.zeros((n, max_chars, char_dimensions))\n",
    "\n",
    "# vectorize output and labels\n",
    "for i, name in enumerate(nicknames):\n",
    "    name = list(name)\n",
    "    for row, ch in enumerate(name):\n",
    "        # assign 1 to nickname, character number, vocab index\n",
    "        output[i, row, char2i[ch]] = 1\n",
    "        \n",
    "    for row, ch in enumerate(nick2real[''.join(name)]):\n",
    "        # assign 1 to real name, character number, vocab index\n",
    "        label[i, row, char2i[ch]] = 1\n",
    "\n",
    "print(output[0])"
   ]
  },
  {
   "source": [
    "As we can see, this is the first nickname that was vectorized. This follows the pattern of having a direct diagnal line of 1's. This is due to our vocabulary setup, as the vocab was pulled directly from our characters, instead of creating a vocabulary on our own.\n",
    "\n",
    "Either way, we know this worked, so lets move on to training!\n",
    "\n",
    "# Training the neural network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<tensorflow.python.keras.engine.sequential.Sequential object at 0x000001F01C971B50>\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(max_chars, char_dimensions), return_sequences=True))\n",
    "model.add(Dense(char_dimensions, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print(model)"
   ]
  },
  {
   "source": [
    "As we can see, we generated the neural model.\n",
    "\n",
    "As we do not care about accuracy in the traditional sense for this task, I will build a simple function to generate a name based on the model probabilities. This will be used to generate names at every designated epoch."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name(model, limit, input):\n",
    "    # vectorize the input\n",
    "    word_vec = np.zeros((1, max_chars, char_dimensions))\n",
    "\n",
    "    def predict(index):\n",
    "        # pull probabilities for character index\n",
    "        probabilities = list(model.predict(word_vec)[0,index])\n",
    "        # normalize probabilities\n",
    "        probabilities = probabilities / np.sum(probabilities)\n",
    "        if index == limit-1:\n",
    "            return '.'\n",
    "        # guess a letter\n",
    "        guess = np.random.choice(range(char_dimensions), p=probabilities) # choose a letter\n",
    "        word_vec[0, index+1, guess] = 1\n",
    "        return i2char[guess]\n",
    "\n",
    "\n",
    "    gen_name = ''.join([predict(i) for i in range(limit)])\n",
    "    print(f'{input}: {gen_name}')"
   ]
  },
  {
   "source": [
    "# Training\n",
    "\n",
    "Now lets run our model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Names generated after epoch 0:\n",
      "donald trump: jannss  fadd.\n",
      "joe biden: jannss fadnd.\n",
      "hillary clinton: cae ck oende.\n",
      "\n",
      "Names generated after epoch 999:\n",
      "donald trump: jaun   famer.\n",
      "joe biden: jannss fadnw.\n",
      "hillary clinton: blexddssknid.\n",
      "\n",
      "Names generated after epoch 1998:\n",
      "donald trump: jarnsr caher.\n",
      "joe biden: jannss hannw.\n",
      "hillary clinton: jaeeeb ffdee.\n",
      "\n",
      "Names generated after epoch 2997:\n",
      "donald trump: jannss cannw.\n",
      "joe biden: jannss farnw.\n",
      "hillary clinton: haalls  hhws.\n",
      "\n",
      "Names generated after epoch 3996:\n",
      "donald trump: jannssssaaaw.\n",
      "joe biden: cae ci conyy.\n",
      "hillary clinton: mooogeenabat.\n",
      "\n",
      "Names generated after epoch 4995:\n",
      "donald trump: aaenss hanni.\n",
      "joe biden: mamnilccomwo.\n",
      "hillary clinton: jaeeni  ende.\n",
      "\n",
      "Names generated after epoch 5994:\n",
      "donald trump: paless hiiio.\n",
      "joe biden: geett  hende.\n",
      "hillary clinton: jannss hannw.\n",
      "\n",
      "Names generated after epoch 6993:\n",
      "donald trump: jannss hanww.\n",
      "joe biden: aalensccciii.\n",
      "hillary clinton: jannss fannw.\n",
      "\n",
      "Names generated after epoch 7992:\n",
      "donald trump: nnnncccfeeee.\n",
      "joe biden: geencc cenee.\n",
      "hillary clinton: aaenss hainn.\n",
      "\n",
      "Names generated after epoch 8991:\n",
      "donald trump: geee  binnns.\n",
      "joe biden: maensscccorr.\n",
      "hillary clinton: palesscciiii.\n",
      "\n",
      "Names generated after epoch 9990:\n",
      "donald trump: samnsssccmmr.\n",
      "joe biden: mamnnle coco.\n",
      "hillary clinton: eeeffffkdeed.\n",
      "\n",
      "INFO:tensorflow:Assets written to: model.output\\assets\n"
     ]
    }
   ],
   "source": [
    "def generate_name_loop(epoch, _):\n",
    "    if epoch % 999 == 0:\n",
    "        \n",
    "        print('Names generated after epoch %d:' % epoch)\n",
    "\n",
    "        for i, name in enumerate(['donald trump', 'joe biden', 'hillary clinton']):\n",
    "            generate_name(model, limit = 13, input = name)\n",
    "        \n",
    "        print()\n",
    "      \n",
    "name_generator = LambdaCallback(on_epoch_end = generate_name_loop)\n",
    "\n",
    "model.fit(output, label, batch_size=64, epochs=10000, callbacks=[name_generator], verbose=0)\n",
    "\n",
    "model.save(\"model.output\")"
   ]
  },
  {
   "source": [
    "Things to note about this training:\n",
    "\n",
    "+ no name was less than the max characters\n",
    "    - this means that every generation never predicted the end of the name itself, this might be able to be rectified using the probabilities based on our data set. However, this is a major limiter into the useability and generation of the names.\n",
    "\n",
    "+ we see names with more than one consecutive space\n",
    "    - this could easily be corrected by hardcoding a limit to one consecutive space in the name generation, however, it is not worth the effort for this model.\n",
    "\n",
    "+ the generated names tend to become more word-like, showing a good understanding of how words are put together.\n",
    "    - However, the biggest problem we see is repeated characters. This could also be hardcoded, but would limit a name that has consecutive letters, such as 'hillary'.\n",
    "    \n",
    "As we can see from the epoch generations, the model does not work well at all. However, I am going to save the model to generate some names and evaluate how well it did.\n",
    "\n",
    "# Model Evaluation\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model(\"model.output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "limit: 5 --------\n",
      "alex kahanek: jann.\n",
      "donald trump: jaee.\n",
      "joe biden: mamn.\n",
      "barack obama: pale.\n",
      "george bush: jann.\n",
      "hillary clinton: jann.\n",
      "karen: ble .\n",
      "bob: jann.\n",
      "the one guy: aaen.\n",
      "dumbo: jann.\n",
      "1% joe: jann.\n",
      "this name is really long: jaen.\n",
      "-----------------------\n",
      "\n",
      "\n",
      "limit: 10 --------\n",
      "alex kahanek: palesscci.\n",
      "donald trump: hhrnllls .\n",
      "joe biden: ble  iinn.\n",
      "barack obama: geett  te.\n",
      "george bush: jannss ca.\n",
      "hillary clinton: laannsscc.\n",
      "karen: geee cidt.\n",
      "bob: jarns  ca.\n",
      "the one guy: jannss  a.\n",
      "dumbo: kiille  u.\n",
      "1% joe: geett  te.\n",
      "this name is really long: aaenss ha.\n",
      "-----------------------\n",
      "\n",
      "\n",
      "limit: 15 --------\n",
      "alex kahanek: jale   feddees.\n",
      "donald trump: aolleeooaaaess.\n",
      "joe biden: blee iinnnnyyr.\n",
      "barack obama: blexd tfffiii .\n",
      "george bush: jannss canwwen.\n",
      "hillary clinton: bjoniiitenrrre.\n",
      "karen: jannss haeedde.\n",
      "bob: jamn   hunrrrn.\n",
      "the one guy: jannsss wdndde.\n",
      "dumbo: nnnnnnccllllll.\n",
      "1% joe: geee cidenidei.\n",
      "this name is really long: aaenssccciiinn.\n",
      "-----------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = 'alex kahanek,donald trump,joe biden,barack obama,george bush,hillary clinton,karen,bob,the one guy,dumbo,1% joe,this name is really long'.split(',')\n",
    "name_limit = [5, 10, 15]\n",
    "\n",
    "for limit in name_limit:\n",
    "    print(f'limit: {limit} --------')\n",
    "    for name in names:\n",
    "        generate_name(model, limit = limit, input = name)\n",
    "    print('-----------------------')\n",
    "    print('\\n')"
   ]
  },
  {
   "source": [
    "From this we can see that the character based name generation did not work well. It seems as though there is not enough training data to generate coherent names, as such I think a word based model might perform better for this task.\n",
    "\n",
    "Notes about why this is bad:\n",
    "\n",
    "+ as the limit increases, we see more repeated characters\n",
    "+ the names never stop themselves\n",
    "+ the algorithm really likes the letter j\n",
    "+ none of the names make any sense\n",
    "\n",
    "Notes about what is good:\n",
    "\n",
    "+ notice that if you took out the repeat letters, the names do resemble words more.\n",
    "+ it does seem to understand the difference and usage of vowels and constants,\n",
    "    - if we ignore the repeat letters.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}