{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit",
   "display_name": "Python 3.8.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "0de36b31320ba4c88b4f85a74724f3d16c36a44df48581253710b1065e752d9e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Training a Nerual Network to generate Trump Nicknames\n",
    "\n",
    "This is going to be a word-based approach to nickname generation. We will be going through the preprocessing required to tokenize the nicknames, based off the data set pulled from [this link here]. The data was cleaned and analyzed by me, click [this link here] to see that analysis.\n",
    "\n",
    "# Grabbing the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       fake name           real name  len fake  len real  \\\n",
       "0          dumbo  randolph tex alles         1         3   \n",
       "1  wheres hunter        hunter biden         2         2   \n",
       "2         1% joe           joe biden         2         2   \n",
       "3   basement joe           joe biden         3         2   \n",
       "4    beijing joe           joe biden         3         2   \n",
       "\n",
       "                     category  \\\n",
       "0  domestic political figures   \n",
       "1  domestic political figures   \n",
       "2  domestic political figures   \n",
       "3  domestic political figures   \n",
       "4  domestic political figures   \n",
       "\n",
       "                                               notes  count  \n",
       "0       director of the united states secret service      1  \n",
       "1  american lawyer and lobbyist who is the second...      1  \n",
       "2  47th vice president of the united states; form...      1  \n",
       "3  47th vice president of the united states; form...      1  \n",
       "4  47th vice president of the united states; form...      1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fake name</th>\n      <th>real name</th>\n      <th>len fake</th>\n      <th>len real</th>\n      <th>category</th>\n      <th>notes</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>dumbo</td>\n      <td>randolph tex alles</td>\n      <td>1</td>\n      <td>3</td>\n      <td>domestic political figures</td>\n      <td>director of the united states secret service</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>wheres hunter</td>\n      <td>hunter biden</td>\n      <td>2</td>\n      <td>2</td>\n      <td>domestic political figures</td>\n      <td>american lawyer and lobbyist who is the second...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1% joe</td>\n      <td>joe biden</td>\n      <td>2</td>\n      <td>2</td>\n      <td>domestic political figures</td>\n      <td>47th vice president of the united states; form...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>basement joe</td>\n      <td>joe biden</td>\n      <td>3</td>\n      <td>2</td>\n      <td>domestic political figures</td>\n      <td>47th vice president of the united states; form...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>beijing joe</td>\n      <td>joe biden</td>\n      <td>3</td>\n      <td>2</td>\n      <td>domestic political figures</td>\n      <td>47th vice president of the united states; form...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 110
    }
   ],
   "source": [
    "# data manip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# model building\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "raw = pd.read_csv('data/cleaned.nicknames.csv')\n",
    "\n",
    "raw.head()"
   ]
  },
  {
   "source": [
    "Here is the data we are working with. We have the nicknames (fake name) and the corrosponding real name of the individual the nickname was given to by trump. We lso have a few other columns, however those will not matter for this task.\n",
    "\n",
    "# Tokenizing the names\n",
    "\n",
    "Here we need to seperate each word and add a tag. We will be adding a few created tags:\n",
    "\n",
    "+ real names:\n",
    "    - these will be given a tag for each word in the name, for example,\n",
    "        + 'joe biden' will become 'joe <name1> biden <name2>'\n",
    "\n",
    "+ nicknames:\n",
    "    - these follow the rule that if a real name is in the nickname, it will be replaced with the corrosponding real name tag.\n",
    "        + ie, `basement joe` will become `basement <name1>`\n",
    "    - a `<prefix>` tag will be added to every other word before the substitution, and a `<suffix>` tag to the substitution and words after.\n",
    "    - if there are no substitutions, then a `<nope>` tag will be added to each word.\n",
    "\n",
    "-----------------------\n",
    "reword this mess\n",
    "\n",
    "These modifications will be made because we will use the generated `<nameX>` tags to use from the user input. For example, if a user inputs `Joe Biden`, and the generated name follows `<prefix> <name1>` the generation algorithm can substitute the users `<name1>` with `joe` in this example, so that all we need to generate is the `<prefix>` tag. Although the model will need to predict a name tag from the suffix part of the nickname. This will help with training, as we only need to predict the length of the nickname, then the tags that follow. For example, if we generate tags as `<prefix> <prefix> <suffix>`, we can generate the best for each category, where the name tags can only come from the `<suffix>` tag."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "real name | nickname\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('joe <name1> biden <name2>',\n",
       "  'sleepy <prefix> creepy <prefix> <name1> <name>'),\n",
       " ('joe <name1> biden <name2>', 'slow <prefix> <name1> <name>'),\n",
       " ('joe <name1> biden <name2>', '<name1> <name> hiden <suffix>'),\n",
       " ('joe <name1> biden <name2>', 'obiden <prefix>'),\n",
       " ('michael <name1> bloomberg <name2>',\n",
       "  'little <prefix> <name1> <name> <name2> <name>')]"
      ]
     },
     "metadata": {},
     "execution_count": 111
    }
   ],
   "source": [
    "def tokenize(realname, nickname):\n",
    "    '''tokenizes reach real name and nickname to follow the rules defined'''\n",
    "\n",
    "    # get a dictionary for the real name and corrospoinding token, ie input\n",
    "    real2token = {word: f'<name{X+1}>' for X, word in enumerate(realname.split(' '))}\n",
    "    # convert dictionary to word tokenized groups and join into single string\n",
    "    real_tokenized = ' '.join([f'{word} {real2token[word]}' for word in real2token])\n",
    "\n",
    "    # change nickname into single string with tokenization and substitution\n",
    "    # grab names to substitute\n",
    "    subs = [sub for sub in realname.split(' ') if sub in nickname]\n",
    "\n",
    "    if len(subs) == 0:\n",
    "        # then there are no splits, tokens are <nope>\n",
    "        nick_tokenized = ' '.join([f'{word} <nope>' for word in nickname.split(' ')])\n",
    "        return (real_tokenized, nick_tokenized)\n",
    "\n",
    "    substituted = ' '.join([word if word not in subs else f'{real2token[word]}' for word in nickname.split(' ')])\n",
    "\n",
    "    tokens = ['<prefix>', '<suffix>', '<name>']\n",
    "    index = 0\n",
    "    tokenized = []\n",
    "\n",
    "    for word in substituted.split(' '):\n",
    "        if 'name' in word:\n",
    "            index = 2\n",
    "\n",
    "        tokenized.append(f'{word} {tokens[index]}')\n",
    "        if index == 2:\n",
    "            index = 1\n",
    "    \n",
    "    nick_tokenized = ' '.join(tokenized)\n",
    "\n",
    "    return (real_tokenized, nick_tokenized)\n",
    "\n",
    "\n",
    "tokenized_names = [tokenize(realname, nickname) for i, nickname, realname in raw[['fake name', 'real name']].itertuples()]\n",
    "\n",
    "print('real name | nickname')\n",
    "tokenized_names[10:15]"
   ]
  },
  {
   "source": [
    "Okay, not that everything is tokenized, we can start to vectorize it!\n",
    "\n",
    "# Vectorizing the tokenized names\n",
    "\n",
    "To vectorize this we need to define our vocabulary, then create a matrix for each name. The matrix will have the maximum token length as rows, and the total vocab words as columns. To vectorize it, we simply put a 1 in the row for the corrosponding token in the vocab index."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n [1. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "##### get probabilities of name length #####\n",
    "name_matrix = raw.groupby(['len real', 'len fake']).agg({'count':'count'}).reset_index()\n",
    "name_matrix = name_matrix[name_matrix['len real'] == 2]\n",
    "p_gen_len = name_matrix[['len fake']]\n",
    "p_gen_len['p'] = name_matrix['count']/name_matrix['count'].sum()\n",
    "\n",
    "##### get vocab ######\n",
    "# flatten paired list to get all names\n",
    "flatten = [name for pair in tokenized_names for name in pair]\n",
    "# make sure the created tokens go first in the vocad dictionaries\n",
    "flatten[:0] = ['<prefix>', '<suffix>', '<name>', '<nope>', '<name1>', '<name2>', '<name3>', '<name4>', '<name5>', '<name6>']\n",
    "\n",
    "###### get dictionaries #########\n",
    "# get dictionary of nicknames to\n",
    "nick2real = {nickname:realname for (realname, nickname) in tokenized_names}\n",
    "# get dictionaries for vocab\n",
    "uni_tokens = {token:0 for name in flatten for token in name.split(' ')}\n",
    "# dictionary for token to index\n",
    "token2i = {token:i for i, token in enumerate(uni_tokens)}\n",
    "# dictionary for index to token\n",
    "i2token = {token2i[token]: token for token in token2i}\n",
    "# dictionary for index to real name token\n",
    "i2name = {token2i[word]:word for pair in tokenized_names for word in pair[0].split(' ') if '<' not in word}\n",
    "\n",
    "######### math ##########\n",
    "# find total number of nicknames\n",
    "n = len(tokenized_names)\n",
    "# find max number of tokens, ie, rows in matrix\n",
    "max_tokens = max([len(name.split(' ')) for name in flatten])\n",
    "# find total number of tokens, ie columns in matrix\n",
    "token_dimensions = len(i2token)\n",
    "\n",
    "##### get matricies ########\n",
    "# set up vectors for output = nicknames\n",
    "output = np.zeros((n, max_tokens, token_dimensions))\n",
    "# set up vectors for label = real names\n",
    "input = np.zeros((n, max_tokens, token_dimensions))\n",
    "\n",
    "#### vectorize names #######\n",
    "for i, nickname in enumerate(nick2real):\n",
    "    # input and output assignment\n",
    "    for row, token in enumerate(nickname.split(' ')):\n",
    "        output[i, row, token2i[token]] = 1\n",
    "        input[i, row, token2i[token]] = 1\n",
    "    \n",
    "    # output assignment\n",
    "    # for row, token in enumerate(nick2real[nickname].split(' ')):\n",
    "    #     label[i, row, token2i[token]] = 1\n",
    "\n",
    "print(output[1])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "source": [
    "Awesome, here we can see a vectorized word! It doesn't look like much because the dimensions are very large, but there is a single 1 at every token index for the row of the token. For example, row 2 in this matrix says the first word is a prefix.\n",
    "\n",
    "From here we need to build a few functions for the model!\n",
    "\n",
    "# Building functions for the model\n",
    "\n",
    "Since this isnt a traditional training, we need to evaluate our model by actually generating nicknames. To do this, I will build a generate function and generate a few names at every nth epoch."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name(model, input):\n",
    "    '''generates a nickname based on length of word and input given'''\n",
    "\n",
    "    word_vec = np.zeros((1, max_tokens, token_dimensions))\n",
    "\n",
    "    p_len = p_gen_len['p']\n",
    "    gen_len = p_gen_len.loc[np.random.choice(range(len(p_len)), p=p_len),'len fake']\n",
    "    # print(gen_len)\n",
    "    # get a dictionary for the real name and corrospoinding token, ie input\n",
    "    token2input = {f'<name{X+1}>': word for X, word in enumerate(input.split(' '))}\n",
    "    # convert dictionary to word tokenized groups and join into single string\n",
    "    input_tokenized = ' '.join([f'{token2input[token]} {token}' for token in token2input])\n",
    "    nName = 0 # tracks the number of name affix given\n",
    "\n",
    "    affixs = []\n",
    "    for index in range(gen_len):\n",
    "        index = (index+1)*2 -1\n",
    "\n",
    "        # pull probabilities for each affix\n",
    "        p_affix = list(model.predict(word_vec)[0,index])[0:4]\n",
    "        # normalize probabilities\n",
    "        p_affix_norm = p_affix / np.sum(p_affix)\n",
    "        # guess a affix\n",
    "        guess = np.random.choice(range(len(p_affix_norm)), p=p_affix_norm)\n",
    "        affix = i2token[guess]\n",
    "\n",
    "        # check affix to make sure names are not chosen more than the given user amount\n",
    "        if 'name' in affix:\n",
    "            if nName < len(token2input):\n",
    "                nName += 1\n",
    "            else:\n",
    "                while 'name' in affix:\n",
    "                    guess = np.random.choice(range(len(p_affix_norm)), p=p_affix_norm) # choose an affix\n",
    "                    affix = i2token[guess]\n",
    "        # assign value to generated name vector\n",
    "        word_vec[0, index, guess] = 1\n",
    "        # print(f'p={p_affix_norm}, g={i2token[guess]}, i={index}')\n",
    "        affixs.append(affix)\n",
    "\n",
    "    def predict_token(index, affix):\n",
    "        '''generates a token based on the given affix''' \n",
    "        # print(f'g={guess}, i={index}')\n",
    "        if 'name' in affix:\n",
    "            # pull probabilities for all names: index 4 - 4+given amount (max ind 9)\n",
    "            p_name = list(model.predict(word_vec)[0,index])[4:4+len(token2input)]\n",
    "            p_name_norm = p_name / np.sum(p_name)\n",
    "            # generate name token\n",
    "            guess = np.random.choice(range(4, len(p_name_norm)+4), p=p_name_norm)\n",
    "\n",
    "            # prevent repeat name tokens\n",
    "            while word_vec[0,index-2,guess] == 1.0:\n",
    "                guess = np.random.choice(range(4, len(p_name_norm)+4), p=p_name_norm)\n",
    "                # print(guess, len(p_name_norm)+4, len(token2input))\n",
    "                # print(i2token[guess], input)\n",
    "            token = i2token[guess]\n",
    "            word = token2input[token]\n",
    "        \n",
    "        else:\n",
    "            # affix is prefix, suffix, or nope\n",
    "            # grab porbabilities of all non affix or name tokens: 10 - end\n",
    "            p_tokens = list(model.predict(word_vec)[0,index])[10:]\n",
    "            p_tokens_norm = p_tokens / np.sum(p_tokens)\n",
    "            # generate a guess from probabilities\n",
    "            guess = np.random.choice(range(10, len(p_tokens_norm)+10), p=p_tokens_norm)\n",
    "\n",
    "            # prevent named vocab from being chosen and repeat tokens\n",
    "            while guess in i2name or word_vec[0,index-2,guess] == 1.0:\n",
    "                # print(i2token[guess])\n",
    "                guess = np.random.choice(range(10, len(p_tokens_norm)+10), p=p_tokens_norm)\n",
    "            word = i2token[guess]\n",
    "            \n",
    "        word_vec[0, index, guess] = 1\n",
    "        return word\n",
    "\n",
    "    # print(f'{input}: {affixs}')\n",
    "    tokens = [predict_token(i*2, affix) for i, affix in enumerate(affixs)]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def generate_name_loop(epoch, _):\n",
    "    '''tells the model when to generate names during training'''\n",
    "    if epoch % (nEpochs//5) == 0 or epoch == nEpochs-1:\n",
    "        \n",
    "        print(f'Nicknames generated on epoch {epoch}')\n",
    "\n",
    "        for i, name in enumerate(['karen','mitch mcconnel','dwayne the rock johnson']):\n",
    "            print(f'{name} | {generate_name(model, name)}')\n",
    "        \n",
    "        print('-------------')"
   ]
  },
  {
   "source": [
    "Awesome, now that those functions are done lets move on to building and training the model. I am going to use a single LSTM "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Nicknames generated on epoch 0\n",
      "karen | wise karen\n",
      "mitch mcconnel | flakey mitch\n",
      "dwayne the rock johnson | slow wise rock\n",
      "-------------\n",
      "Nicknames generated on epoch 150\n",
      "karen | karen lockheed pakistani for\n",
      "mitch mcconnel | goofy wise mitch\n",
      "dwayne the rock johnson | rock\n",
      "-------------\n",
      "Nicknames generated on epoch 300\n",
      "karen | karen\n",
      "mitch mcconnel | mitch sick\n",
      "dwayne the rock johnson | michigan the\n",
      "-------------\n",
      "Nicknames generated on epoch 450\n",
      "karen | energy karen nang rocket\n",
      "mitch mcconnel | flakey braindead\n",
      "dwayne the rock johnson | sneaky lockheed the 38\n",
      "-------------\n",
      "Nicknames generated on epoch 600\n",
      "karen | shifty giuseppi\n",
      "mitch mcconnel | mr mitch\n",
      "dwayne the rock johnson | mad dwayne the dwayne the\n",
      "-------------\n",
      "Nicknames generated on epoch 749\n",
      "karen | slimeball karen senator crazy\n",
      "mitch mcconnel | original wise mitch pocahontas\n",
      "dwayne the rock johnson | on neuman for\n",
      "-------------\n",
      "INFO:tensorflow:Assets written to: models/word/model.bs120.e750.nL25.output\\assets\n"
     ]
    }
   ],
   "source": [
    "# model attributes\n",
    "nBatch = 120 # number of sample in training epoch\n",
    "nEpochs = 750 # number of epochs to train\n",
    "nUnits = 25 # number of loops in lstm\n",
    "\n",
    "model = Sequential()\n",
    "# add LSTM layer to model\n",
    "model.add(LSTM(nUnits, input_shape=(max_tokens, token_dimensions), return_sequences=True))\n",
    "# add model attributes\n",
    "model.add(Dense(token_dimensions, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# generate names on epochs\n",
    "name_generator = LambdaCallback(on_epoch_end = generate_name_loop)\n",
    "\n",
    "model.fit(output, input, batch_size=nBatch, epochs=nEpochs, callbacks=[name_generator], verbose=0)\n",
    "\n",
    "model.save(f\"models/word/model.bs{nBatch}.e{nEpochs}.nL{nUnits}.output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "model.bs120.e750.nL25.output\n",
      "karen | britain karen canada\n",
      "karen | quid karen senator dog\n",
      "karen | apple karen\n",
      "karen | cryin karen apple\n",
      "karen | little guy\n",
      "-----------------------\n",
      "Link | wild Link slow\n",
      "Link | high Link corrupt pounce kellyanne\n",
      "Link | tax Link half a\n",
      "Link | for senator\n",
      "Link | the Link\n",
      "-----------------------\n",
      "Donald trump | 38 iq Donald trump\n",
      "Donald trump | apple lyin\n",
      "Donald trump | sleepy trump half\n",
      "Donald trump | slow Donald\n",
      "Donald trump | sloppy from tough peron man\n",
      "-----------------------\n",
      "Joe Biden | no Biden\n",
      "Joe Biden | phony senator Joe hiden\n",
      "Joe Biden | flailer Biden Joe\n",
      "Joe Biden | the Biden\n",
      "Joe Biden | foul Biden tough\n",
      "-----------------------\n",
      "mitch mcconnel | lying mitch mcconnel apple\n",
      "mitch mcconnel | duarte mcconnel\n",
      "mitch mcconnel | wild\n",
      "mitch mcconnel | crime mitch\n",
      "mitch mcconnel | flunkie mcmuffin\n",
      "-----------------------\n",
      "alexander ray kahanek | 38 for dog professor\n",
      "alexander ray kahanek | fredo mystery\n",
      "alexander ray kahanek | sir ray dictator\n",
      "alexander ray kahanek | lightweight sick rocket\n",
      "alexander ray kahanek | da wise chin\n",
      "-----------------------\n",
      "dwayne the rock johnson | low the atheist the\n",
      "dwayne the rock johnson | mini puppet\n",
      "dwayne the rock johnson | little da\n",
      "dwayne the rock johnson | 0% dwayne the\n",
      "dwayne the rock johnson | wannabe dwayne\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model(f\"models/word/model.bs{nBatch}.e{nEpochs}.nL{nUnits}.output\")\n",
    "\n",
    "names = 'karen,Link,Donald trump,Joe Biden,mitch mcconnel,alexander ray kahanek,dwayne the rock johnson'.split(',')\n",
    "\n",
    "print(f'model.bs{nBatch}.e{nEpochs}.nL{nUnits}.output')\n",
    "for name in names:\n",
    "    for i in range(5):\n",
    "        print(f'{name} | {generate_name(model, name)}')\n",
    "    print('-----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: models/word/keep/model.bs30.e500.nL50.output\\assets\n"
     ]
    }
   ],
   "source": [
    "# us this if want to save model in good\n",
    "model.save(f\"models/word/keep/model.bs{nBatch}.e{nEpochs}.nL{nUnits}.output\")"
   ]
  },
  {
   "source": [
    "add length of nickname to vector?\n",
    "\n",
    "figure out how to jump names, ie if name1 then name2\n",
    "figure out how to stop guessing names, ie dictionary of possible words? pre, suff, nope dictionary?\n",
    "probabilities for generated name length"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}