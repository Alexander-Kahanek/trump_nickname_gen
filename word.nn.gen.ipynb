{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit",
   "display_name": "Python 3.8.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "0de36b31320ba4c88b4f85a74724f3d16c36a44df48581253710b1065e752d9e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Training a Nerual Network to generate Trump Nicknames\n",
    "\n",
    "This is going to be a word-based approach to nickname generation. We will be going through the preprocessing required to tokenize the nicknames, based off the data set pulled from [this link here]. The data was cleaned and analyzed by me, click [this link here] to see that analysis.\n",
    "\n",
    "# Grabbing the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       fake name           real name  len fake  len real  \\\n",
       "0          dumbo  randolph tex alles         1         3   \n",
       "1  wheres hunter        hunter biden         2         2   \n",
       "2         1% joe           joe biden         2         2   \n",
       "3   basement joe           joe biden         3         2   \n",
       "4    beijing joe           joe biden         3         2   \n",
       "\n",
       "                     category  \\\n",
       "0  domestic political figures   \n",
       "1  domestic political figures   \n",
       "2  domestic political figures   \n",
       "3  domestic political figures   \n",
       "4  domestic political figures   \n",
       "\n",
       "                                               notes  count  \n",
       "0       director of the united states secret service      1  \n",
       "1  american lawyer and lobbyist who is the second...      1  \n",
       "2  47th vice president of the united states; form...      1  \n",
       "3  47th vice president of the united states; form...      1  \n",
       "4  47th vice president of the united states; form...      1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fake name</th>\n      <th>real name</th>\n      <th>len fake</th>\n      <th>len real</th>\n      <th>category</th>\n      <th>notes</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>dumbo</td>\n      <td>randolph tex alles</td>\n      <td>1</td>\n      <td>3</td>\n      <td>domestic political figures</td>\n      <td>director of the united states secret service</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>wheres hunter</td>\n      <td>hunter biden</td>\n      <td>2</td>\n      <td>2</td>\n      <td>domestic political figures</td>\n      <td>american lawyer and lobbyist who is the second...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1% joe</td>\n      <td>joe biden</td>\n      <td>2</td>\n      <td>2</td>\n      <td>domestic political figures</td>\n      <td>47th vice president of the united states; form...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>basement joe</td>\n      <td>joe biden</td>\n      <td>3</td>\n      <td>2</td>\n      <td>domestic political figures</td>\n      <td>47th vice president of the united states; form...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>beijing joe</td>\n      <td>joe biden</td>\n      <td>3</td>\n      <td>2</td>\n      <td>domestic political figures</td>\n      <td>47th vice president of the united states; form...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "raw = pd.read_csv('cleaned.nicknames.csv')\n",
    "\n",
    "raw.head()"
   ]
  },
  {
   "source": [
    "Here is the data we are working with. We have the nicknames (fake name) and the corrosponding real name of the individual the nickname was given to by trump. We lso have a few other columns, however those will not matter for this task.\n",
    "\n",
    "# Tokenizing the names\n",
    "\n",
    "Here we need to seperate each word and add a tag. We will be adding a few created tags:\n",
    "\n",
    "+ real names:\n",
    "    - these will be given a tag for each word in the name, for example,\n",
    "        + 'joe biden' will become 'joe <name1> biden <name2>'\n",
    "\n",
    "+ nicknames:\n",
    "    - these follow the rule that if a real name is in the nickname, it will be replaced with the corrosponding real name tag.\n",
    "        + ie, `basement joe` will become `basement <name1>`\n",
    "    - a `<prefix>` tag will be added to every other word before the substitution, and a `<suffix>` tag to the substitution and words after.\n",
    "    - if there are no substitutions, then a `<nope>` tag will be added to each word.\n",
    "\n",
    "-----------------------\n",
    "reword this mess\n",
    "\n",
    "These modifications will be made because we will use the generated `<nameX>` tags to use from the user input. For example, if a user inputs `Joe Biden`, and the generated name follows `<prefix> <name1>` the generation algorithm can substitute the users `<name1>` with `joe` in this example, so that all we need to generate is the `<prefix>` tag. Although the model will need to predict a name tag from the suffix part of the nickname. This will help with training, as we only need to predict the length of the nickname, then the tags that follow. For example, if we generate tags as `<prefix> <prefix> <suffix>`, we can generate the best for each category, where the name tags can only come from the `<suffix>` tag."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "real name | nickname\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('randolph <name1> tex <name2> alles <name3>', 'dumbo <nope>'),\n",
       " ('hunter <name1> biden <name2>', 'wheres <prefix> <name1> <suffix>'),\n",
       " ('joe <name1> biden <name2>', '1% <prefix> <name1> <suffix>')]"
      ]
     },
     "metadata": {},
     "execution_count": 261
    }
   ],
   "source": [
    "def tokenize(realname, nickname):\n",
    "    '''tokenizes reach real name and nickname to follow the rules defined'''\n",
    "\n",
    "    # get a dictionary for the real name and corrospoinding token, ie input\n",
    "    real2token = {word: f'<name{X+1}>' for X, word in enumerate(realname.split(' '))}\n",
    "    # convert dictionary to word tokenized groups and join into single string\n",
    "    real_tokenized = ' '.join([f'{word} {real2token[word]}' for word in real2token])\n",
    "\n",
    "    # change nickname into single string with tokenization and substitution\n",
    "    # grab names to substitute\n",
    "    subs = [sub for sub in realname.split(' ') if sub in nickname]\n",
    "\n",
    "    if len(subs) == 0:\n",
    "        # then there are no splits, tokens are <nope>\n",
    "        nick_tokenized = ' '.join([f'{word} <nope>' for word in nickname.split(' ')])\n",
    "        return (real_tokenized, nick_tokenized)\n",
    "\n",
    "    substituted = ' '.join([word if word not in subs else f'{real2token[word]}' for word in nickname.split(' ')])\n",
    "\n",
    "    token = '<prefix>'\n",
    "    tokenized = []\n",
    "\n",
    "    for word in substituted.split(' '):\n",
    "        if '<' in word and 'prefix' in token:\n",
    "            token = '<suffix>'\n",
    "\n",
    "        tokenized.append(f'{word} {token}')\n",
    "    \n",
    "    nick_tokenized = ' '.join(tokenized)\n",
    "\n",
    "    return (real_tokenized, nick_tokenized)\n",
    "\n",
    "\n",
    "tokenized_names = [tokenize(realname, nickname) for i, nickname, realname in raw[['fake name', 'real name']].itertuples()]\n",
    "\n",
    "print('real name | nickname')\n",
    "tokenized_names[0:3]"
   ]
  },
  {
   "source": [
    "Okay, not that everything is tokenized, we can start to vectorize it!\n",
    "\n",
    "# Vectorizing the tokenized names\n",
    "\n",
    "To vectorize this we need to define our vocabulary, then create a matrix for each name. The matrix will have the maximum token length as rows, and the total vocab words as columns. To vectorize it, we simply put a 1 in the row for the corrosponding token in the vocab index."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n [1. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "##### get vocab ######\n",
    "# flatten paired list to get all names\n",
    "flatten = [name for pair in tokenized_names for name in pair]\n",
    "# flatten = [nick for (real, nick) in tokenized_names]\n",
    "# make sure the created tokens go first in the vocad dictionaries\n",
    "flatten[:0] = ['<prefix>', '<suffix>', '<nope>'] #, '<name1>', '<name2>', '<name3>', '<name4>', '<name5>', '<name6>']\n",
    "\n",
    "###### get dictionaries #########\n",
    "# get dictionary of nicknames to\n",
    "nick2real = {nickname:realname for (realname, nickname) in tokenized_names}\n",
    "# get dictionaries for vocab\n",
    "uni_tokens = {token:0 for name in flatten for token in name.split(' ')}\n",
    "# dictionary for token to index\n",
    "token2i = {token:i for i, token in enumerate(uni_tokens)}\n",
    "# dictionary for index to token\n",
    "i2token = {token2i[token]: token for token in token2i}\n",
    "# dictionary for word to affix\n",
    "\n",
    "######### math ##########\n",
    "# find total number of nicknames\n",
    "n = len(tokenized_names)\n",
    "# find max number of tokens, ie, rows in matrix\n",
    "max_tokens = max([len(name.split(' ')) for name in flatten])\n",
    "# find total number of tokens, ie columns in matrix\n",
    "token_dimensions = len(i2token)\n",
    "\n",
    "##### get matricies ########\n",
    "# set up vectors for output = nicknames\n",
    "output = np.zeros((n, max_tokens, token_dimensions))\n",
    "# set up vectors for label = real names\n",
    "label = np.zeros((n, max_tokens, token_dimensions))\n",
    "\n",
    "#### vectorize names #######\n",
    "for i, nickname in enumerate(nick2real):\n",
    "    # input assignment\n",
    "    for row, token in enumerate(nickname.split(' ')):\n",
    "        output[i, row, token2i[token]] = 1\n",
    "        label[i, row, token2i[token]] = 1\n",
    "    \n",
    "    # output assignment\n",
    "    # for row, token in enumerate(nick2real[nickname].split(' ')):\n",
    "    #     label[i, row, token2i[token]] = 1\n",
    "\n",
    "print(output[1])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "source": [
    "Awesome, here we can see a vectorized word! It doesn't look like much because the dimensions are very large, but there is a single 1 at every character index for the row of the token.\n",
    "\n",
    "From here we need to build the model and train it!\n",
    "\n",
    "# Building the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<tensorflow.python.keras.engine.sequential.Sequential object at 0x000002324CC99CD0>\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import LambdaCallback\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(max_tokens, token_dimensions), return_sequences=True))\n",
    "model.add(Dense(token_dimensions, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "print(model)"
   ]
  },
  {
   "source": [
    "Now that the model is built, we need to build a couple functions. Since this isnt a traditional training, we need to evaluate our model by actually generating nicknames. To do this, I will build a generate function and generate a few names at every nth epoch.\n",
    "\n",
    "# Functions and training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name(model, gen_len, input):\n",
    "    '''generates a nickname based on length of word and input given'''\n",
    "\n",
    "    word_vec = np.zeros((1, max_tokens, token_dimensions))\n",
    "\n",
    "    # get a dictionary for the real name and corrospoinding token, ie input\n",
    "    token2input = {f'<name{X+1}>': word for X, word in enumerate(input.split(' '))}\n",
    "    # convert dictionary to word tokenized groups and join into single string\n",
    "    input_tokenized = ' '.join([f'{token2input[token]} {token}' for token in token2input])\n",
    "\n",
    "    def predict_affix(index):\n",
    "        '''(index+1)*2 to predict affix tags'''\n",
    "        # pull probabilities for character index\n",
    "        p_affix = list(model.predict(word_vec)[0,index])[0:3]\n",
    "        # normalize probabilities\n",
    "        p_affix_norm = p_affix / np.sum(p_affix)\n",
    "\n",
    "        # guess a letter\n",
    "        guess = np.random.choice(range(len(p_affix_norm)), p=p_affix_norm) # choose an affix\n",
    "        word_vec[0, index, guess] = 1\n",
    "        # print(f'p={p_affix_norm}, g={i2token[guess]}, i={index}')\n",
    "        return i2token[guess]\n",
    "\n",
    "    def predict_token(index, affix):\n",
    "        p_token = list(model.predict(word_vec)[0,index])[4:]\n",
    "        p_token_norm = p_token / np.sum(p_token)\n",
    "\n",
    "        guess = np.random.choice(range(4, len(p_token_norm)+4), p=p_token_norm)\n",
    "        word = i2token[guess]\n",
    "        # print(f'g={guess}, i={index}')\n",
    "        if 'name' in word:\n",
    "            if word not in input_tokenized:\n",
    "                word = '<name1>'\n",
    "            guess = token2i[word]\n",
    "            word = token2input[word]\n",
    "            \n",
    "\n",
    "        word_vec[0, index, guess] = 1\n",
    "        return word\n",
    "\n",
    "\n",
    "    affix = [predict_affix((i+1)*2 -1) for i in range(gen_len)]\n",
    "    print(f'{input}: {affix}')\n",
    "    tokens = [predict_token(i*2, affix) for i, affix in enumerate(affix)]\n",
    "    print(f'{input}: {\" \".join(tokens)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Names generated after epoch 0:\n",
      "donald trump: ['<nope>', '<nope>', '<nope>']\n",
      "donald trump: wacky for tax\n",
      "-------------\n",
      "Names generated after epoch 10:\n",
      "donald trump: ['<nope>', '<nope>', '<nope>']\n",
      "donald trump: morning psycho flailer\n",
      "-------------\n",
      "Names generated after epoch 20:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: trump trump wannabe\n",
      "-------------\n",
      "Names generated after epoch 30:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: schitt donald half\n",
      "-------------\n",
      "Names generated after epoch 40:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: record trump trump\n",
      "-------------\n",
      "Names generated after epoch 50:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: little donald donald\n",
      "-------------\n",
      "Names generated after epoch 60:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: for tax canada\n",
      "-------------\n",
      "Names generated after epoch 70:\n",
      "donald trump: ['<nope>', '<nope>', '<nope>']\n",
      "donald trump: wannabe h flunkie\n",
      "-------------\n",
      "Names generated after epoch 80:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: punchy woman \n",
      "-------------\n",
      "Names generated after epoch 90:\n",
      "donald trump: ['<nope>', '<nope>', '<nope>']\n",
      "donald trump: liddle nang wannabe\n",
      "-------------\n",
      "Names generated after epoch 100:\n",
      "donald trump: ['<nope>', '<nope>', '<nope>']\n",
      "donald trump: energy wise hell\n",
      "-------------\n",
      "Names generated after epoch 110:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: man donald sham\n",
      "-------------\n",
      "Names generated after epoch 120:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: boot edge edge\n",
      "-------------\n",
      "Names generated after epoch 130:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: animal donald trump\n",
      "-------------\n",
      "Names generated after epoch 140:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: sir donald trump\n",
      "-------------\n",
      "Names generated after epoch 150:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: corker donald trump\n",
      "-------------\n",
      "Names generated after epoch 160:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: karl donald trump\n",
      "-------------\n",
      "Names generated after epoch 170:\n",
      "donald trump: ['<nope>', '<nope>', '<nope>']\n",
      "donald trump: uber h flunkie\n",
      "-------------\n",
      "Names generated after epoch 180:\n",
      "donald trump: ['<nope>', '<nope>', '<nope>']\n",
      "donald trump: cryin weird wise\n",
      "-------------\n",
      "Names generated after epoch 190:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: sloppy donald trump\n",
      "-------------\n",
      "Names generated after epoch 200:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: original weird trump\n",
      "-------------\n",
      "Names generated after epoch 210:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: donald mcmuffin trump\n",
      "-------------\n",
      "Names generated after epoch 220:\n",
      "donald trump: ['<prefix>', '<prefix>', '<suffix>']\n",
      "donald trump: 0% donald trump\n",
      "-------------\n",
      "Names generated after epoch 230:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: rejected senator senator\n",
      "-------------\n",
      "Names generated after epoch 240:\n",
      "donald trump: ['<nope>', '<nope>', '<nope>']\n",
      "donald trump: mad dog dog\n",
      "-------------\n",
      "Names generated after epoch 250:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: iq donald trump\n",
      "-------------\n",
      "Names generated after epoch 260:\n",
      "donald trump: ['<nope>', '<nope>', '<nope>']\n",
      "donald trump: donald bozo the\n",
      "-------------\n",
      "Names generated after epoch 270:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: cryin donald trump\n",
      "-------------\n",
      "Names generated after epoch 280:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: mini mike trump\n",
      "-------------\n",
      "Names generated after epoch 290:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: michigan trump trump\n",
      "-------------\n",
      "Names generated after epoch 300:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: rocket man \n",
      "-------------\n",
      "Names generated after epoch 310:\n",
      "donald trump: ['<nope>', '<nope>', '<prefix>']\n",
      "donald trump: juan trump trump\n",
      "-------------\n",
      "Names generated after epoch 320:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: southern hemisphere the\n",
      "-------------\n",
      "Names generated after epoch 330:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: china donald trump\n",
      "-------------\n",
      "Names generated after epoch 340:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: rock donald trump\n",
      "-------------\n",
      "Names generated after epoch 350:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: the flunky trump\n",
      "-------------\n",
      "Names generated after epoch 360:\n",
      "donald trump: ['<nope>', '<nope>', '<nope>']\n",
      "donald trump: crime h flunkie\n",
      "-------------\n",
      "Names generated after epoch 370:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: beijing donald trump\n",
      "-------------\n",
      "Names generated after epoch 380:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: gov fat donald\n",
      "-------------\n",
      "Names generated after epoch 390:\n",
      "donald trump: ['<nope>', '<nope>', '<nope>']\n",
      "donald trump: skank 13 man\n",
      "-------------\n",
      "Names generated after epoch 400:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: little senator trump\n",
      "-------------\n",
      "Names generated after epoch 410:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: 13 antoinette trump\n",
      "-------------\n",
      "Names generated after epoch 420:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: mr kellyanne trump\n",
      "-------------\n",
      "Names generated after epoch 430:\n",
      "donald trump: ['<suffix>', '<suffix>', '<suffix>']\n",
      "donald trump: little wise flunky\n",
      "-------------\n",
      "Names generated after epoch 440:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: mr kellyanne trump\n",
      "-------------\n",
      "Names generated after epoch 450:\n",
      "donald trump: ['<nope>', '<nope>', '<nope>']\n",
      "donald trump: truly woman from\n",
      "-------------\n",
      "Names generated after epoch 460:\n",
      "donald trump: ['<suffix>', '<suffix>', '<suffix>']\n",
      "donald trump: for from hell\n",
      "-------------\n",
      "Names generated after epoch 470:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: cheatin trump trump\n",
      "-------------\n",
      "Names generated after epoch 480:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: energy donald trump\n",
      "-------------\n",
      "Names generated after epoch 490:\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: creepy donald trump\n",
      "-------------\n",
      "INFO:tensorflow:Assets written to: word.model.output\\assets\n"
     ]
    }
   ],
   "source": [
    "def generate_name_loop(epoch, _):\n",
    "    if epoch % 10 == 0:\n",
    "        \n",
    "        print('Names generated after epoch %d:' % epoch)\n",
    "\n",
    "        for i, name in enumerate(['donald trump']):\n",
    "            generate_name(model, gen_len = 3, input = name)\n",
    "        \n",
    "        print('-------------')\n",
    "      \n",
    "name_generator = LambdaCallback(on_epoch_end = generate_name_loop)\n",
    "\n",
    "model.fit(output, label, batch_size=64, epochs=500, callbacks=[name_generator], verbose=0)\n",
    "\n",
    "model.save(\"word.model.output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "limit: 2 --------\n",
      "alex kahanek: ['<prefix>', '<suffix>']\n",
      "alex kahanek: hiden nang\n",
      "donald trump: ['<nope>', '<nope>']\n",
      "donald trump: corrupt for\n",
      "joe biden: ['<prefix>', '<suffix>']\n",
      "joe biden: cuban joe\n",
      "barack obama: ['<nope>', '<nope>']\n",
      "barack obama: dicky mcmuffin\n",
      "this name is really long: ['<prefix>', '<suffix>']\n",
      "this name is really long: sleepy creepy\n",
      "-----------------------\n",
      "\n",
      "\n",
      "limit: 3 --------\n",
      "alex kahanek: ['<prefix>', '<suffix>', '<suffix>']\n",
      "alex kahanek: flunky crime kahanek\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>']\n",
      "donald trump: the from sham\n",
      "joe biden: ['<prefix>', '<suffix>', '<suffix>']\n",
      "joe biden: slimeball biden biden\n",
      "barack obama: ['<nope>', '<nope>', '<nope>']\n",
      "barack obama: 0% h the\n",
      "this name is really long: ['<nope>', '<nope>', '<nope>']\n",
      "this name is really long: dopey cnn flunky\n",
      "-----------------------\n",
      "\n",
      "\n",
      "limit: 4 --------\n",
      "alex kahanek: ['<prefix>', '<suffix>', '<suffix>', '<suffix>']\n",
      "alex kahanek: sick puppy a nancy\n",
      "donald trump: ['<prefix>', '<suffix>', '<suffix>', '<suffix>']\n",
      "donald trump: slow donald trump trump\n",
      "joe biden: ['<prefix>', '<suffix>', '<suffix>', '<suffix>']\n",
      "joe biden: dick biden biden biden\n",
      "barack obama: ['<nope>', '<nope>', '<nope>', '<nope>']\n",
      "barack obama: cutie pie pie pie\n",
      "this name is really long: ['<nope>', '<nope>', '<nope>', '<nope>']\n",
      "this name is really long: snowman woman  \n",
      "-----------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model(\"word.model.output\")\n",
    "\n",
    "names = 'alex kahanek,donald trump,joe biden,barack obama,this name is really long'.split(',')\n",
    "gen_len = [2, 3, 4]\n",
    "\n",
    "for limit in gen_len:\n",
    "    print(f'limit: {limit} --------')\n",
    "    for name in names:\n",
    "        generate_name(model, gen_len = limit, input = name)\n",
    "    print('-----------------------')\n",
    "    print('\\n')"
   ]
  },
  {
   "source": [
    "add length of nickname to vector?\n",
    "\n",
    "figure out how to jump names, ie if name1 then name2\n",
    "figure out how to stop guessing names, ie dictionary of possible words? pre, suff, nope dictionary?\n",
    "probabilities for generated name length"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}